{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Example 1: Data Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the training data:\n",
      "[[4.6 3.6 1.  0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.4 3.2 1.3 0.2]]\n",
      "\n",
      "\n",
      " First 5 rows of the scaled training data:\n",
      "[[-1.47393679  1.20365799 -1.56253475 -1.31260282]\n",
      " [-0.13307079  2.99237573 -1.27600637 -1.04563275]\n",
      " [ 1.08589829  0.08570939  0.38585821  0.28921757]\n",
      " [-1.23014297  0.75647855 -1.2187007  -1.31260282]\n",
      " [-1.7177306   0.30929911 -1.39061772 -1.31260282]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load iris data for classification\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#Before Standardize features\n",
    "print(\"First 5 rows of the training data:\")\n",
    "print(X_train[:5])\n",
    "\n",
    "# Standardize features (zero mean, unit variance)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n\\n First 5 rows of the scaled training data:\")\n",
    "print(X_train_scaled[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Example 2: Simple Imputer (Handling Missing Values)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "\n",
      "   Feature1  Feature2\n",
      "0       1.0       NaN\n",
      "1       2.0       2.0\n",
      "2       NaN       3.0\n",
      "3       4.0       4.0\n",
      "\n",
      "DataFrame after imputing missing values:\n",
      "\n",
      "   Feature1  Feature2\n",
      "0  1.000000       3.0\n",
      "1  2.000000       2.0\n",
      "2  2.333333       3.0\n",
      "3  4.000000       4.0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Creating a sample DataFrame with missing values\n",
    "data = {\n",
    "    'Feature1': [1, 2, np.nan, 4],\n",
    "    'Feature2': [np.nan, 2, 3, 4]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\\n\")\n",
    "print(df)\n",
    "\n",
    "# Initialize the SimpleImputer with a strategy to fill missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit the imputer on the DataFrame and transform it\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "# Convert the result back to a DataFrame\n",
    "df_imputed = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "\n",
    "# Display the DataFrame after imputing missing values\n",
    "print(\"\\nDataFrame after imputing missing values:\\n\")\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Example 3: OneHotEncoder and LabelEncoder</h3>\n",
    "\n",
    "<b>OneHotEncoder</b> converts categorical variables into a format vector that can be provided to machine learning algorithms to do a better job in prediction. One of the main advantages of OneHotEncoder is that it avoids introducing ordinal relationships between categorical variables that do not have any intrinsic order. By converting categories into a binary vector, it ensures that each category is equally distinct and independent.\n",
    "\n",
    "<b>LabelEncoder</b> converts categorical labels into numeric form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "  Category  Value\n",
      "0        A     10\n",
      "1        B     20\n",
      "2        A     10\n",
      "3        C     30 \n",
      "\n",
      "\n",
      "encoded\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]] \n",
      "\n",
      "\n",
      "['Category_A' 'Category_B' 'Category_C']\n",
      "OneHotEncoded DataFrame:\n",
      "   Category_A  Category_B  Category_C\n",
      "0         1.0         0.0         0.0\n",
      "1         0.0         1.0         0.0\n",
      "2         1.0         0.0         0.0\n",
      "3         0.0         0.0         1.0 \n",
      "\n",
      "\n",
      "DataFrame after OneHotEncoding:\n",
      "   Value  Category_A  Category_B  Category_C\n",
      "0     10         1.0         0.0         0.0\n",
      "1     20         0.0         1.0         0.0\n",
      "2     10         1.0         0.0         0.0\n",
      "3     30         0.0         0.0         1.0 \n",
      "\n",
      "\n",
      "Encoded labels:\n",
      "[0 1 0 2] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Creating a sample DataFrame with categorical values\n",
    "data = {\n",
    "    'Category': ['A', 'B', 'A', 'C'],\n",
    "    'Value': [10, 20, 10, 30]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df, \"\\n\\n\")\n",
    "\n",
    "# OneHotEncoder for categorical variables\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded = encoder.fit_transform(df[['Category']])\n",
    "\n",
    "print(\"encoded\")\n",
    "print(encoded,\"\\n\\n\")\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(['Category']))\n",
    "\n",
    "# Display the OneHotEncoded DataFrame\n",
    "print(\"OneHotEncoded DataFrame:\")\n",
    "print(encoded_df, \"\\n\\n\")\n",
    "\n",
    "# Concatenate the encoded columns with the original DataFrame\n",
    "df_onehot = pd.concat([df, encoded_df], axis=1).drop('Category', axis=1)\n",
    "\n",
    "# Display the DataFrame after OneHotEncoding\n",
    "print(\"DataFrame after OneHotEncoding:\")\n",
    "print(df_onehot, \"\\n\\n\")\n",
    "\n",
    "# Creating a sample array with categorical labels\n",
    "labels = ['A', 'B', 'A', 'C']\n",
    "\n",
    "# LabelEncoder for labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Display the encoded labels\n",
    "print(\"Encoded labels:\")\n",
    "print(y_encoded, \"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Pipeline for Preprocessing and Model Training\n",
    "\n",
    "In this example, we will create a preprocessing pipeline that includes handling missing values and scaling features. We will then add a classifier to this pipeline and train the model.\n",
    "\n",
    "#### Steps:\n",
    "1. Create a preprocessing pipeline for numeric features.\n",
    "2. Combine preprocessing and classifier into one pipeline.\n",
    "3. Train the model using the pipeline.\n",
    "4. Predict the test set results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels:\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load iris data for classification\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating a preprocessing pipeline\n",
    "# Define a pipeline for numeric features: imputing missing values with median and then scaling the features\n",
    "numeric_features = [0, 1, 2, 3]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Step to handle missing values by replacing them with the median value\n",
    "    ('scaler', StandardScaler())  # Step to standardize features by removing the mean and scaling to unit variance\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps using ColumnTransformer to apply the numeric transformer to the numeric features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('my_numeric_transformer', numeric_transformer, numeric_features)  # Apply the numeric transformer to the specified numeric features\n",
    "    ])\n",
    "\n",
    "# Creating a complete pipeline that includes both the preprocessor and the classifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # First apply the preprocessor to the data\n",
    "    ('classifier', RandomForestClassifier(random_state=42))  # Then apply the classifier to the preprocessed data\n",
    "])\n",
    "\n",
    "# Training the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Displaying the predicted results\n",
    "print(\"Predicted labels:\")\n",
    "print(y_pred, \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Comprehensive Pipeline Example ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels:\n",
      "[1 0 2 1 2 0 1 2 2 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0] \n",
      "\n",
      "\n",
      "Actual labels:\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load iris data for classification\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# Creating a DataFrame to introduce categorical data for encoding step\n",
    "df = pd.DataFrame(X_iris, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "df['category'] = np.random.choice(['A', 'B', 'C'], size=df.shape[0])  # Adding a categorical feature\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y_iris, test_size=0.2, random_state=42)\n",
    "\n",
    "# Numeric and categorical features\n",
    "numeric_features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "categorical_features = ['category']\n",
    "\n",
    "# Creating preprocessing pipelines for numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Step to handle missing values by replacing them with the median value\n",
    "    ('scaler', StandardScaler()),  # Step to standardize features by removing the mean and scaling to unit variance\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False))  # Step to generate polynomial and interaction features\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # Step to handle missing values by replacing them with a constant value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Step to encode categorical variables as binary vectors\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps using ColumnTransformer to apply the numeric and categorical transformers to the respective features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),  # Apply the numeric transformer to the specified numeric features\n",
    "        ('cat', categorical_transformer, categorical_features)  # Apply the categorical transformer to the specified categorical features\n",
    "    ])\n",
    "\n",
    "# Creating a complete pipeline that includes preprocessing, PCA, feature selection, and classification\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # Apply the preprocessor to the data\n",
    "    ('pca', PCA(n_components=5)),  # Reduce the number of features to 5 principal components\n",
    "    ('select', SelectKBest(score_func=f_classif, k=5)),  # Select the top 5 features based on ANOVA F-value\n",
    "    ('classifier', RandomForestClassifier(random_state=42))  # Apply the RandomForestClassifier to the preprocessed data\n",
    "])\n",
    "\n",
    "# Training the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Displaying the predicted results\n",
    "print(\"Predicted labels:\")\n",
    "print(y_pred, \"\\n\\n\")\n",
    "\n",
    "# Displaying the actual labels\n",
    "print(\"Actual labels:\")\n",
    "print(y_test, \"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear regresion Algorithm ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.5558915986952424\n",
      "Mean Absolute Error (MAE): 0.5332001304956981\n",
      "R-squared: 0.5757877060324523\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load California housing data\n",
    "california_housing = fetch_california_housing()\n",
    "X = california_housing.data\n",
    "y = california_housing.target\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "# Calculate R-squared (coefficient of determination)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
